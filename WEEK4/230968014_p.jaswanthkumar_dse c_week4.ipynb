{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c54bf5-9b7a-41ab-967c-ddffd03831f7",
   "metadata": {},
   "source": [
    "# WEEK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3eba9-1623-4cae-a4ce-9c12075ebf67",
   "metadata": {},
   "source": [
    "# Cats and Dogs Dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e15daf-cd49-49f5-af2d-228640335c50",
   "metadata": {},
   "source": [
    "# Q1. Implementation and Comparison of CNN Architectures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "00e26e86-44df-4495-941a-1f939523127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a43ed1a2-202f-4c0f-87d5-38f4152c15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/mca/230968014_p.jaswanthkumar/week4/cats_and_dogs_filtered/train\"\n",
    "val_dir   = \"/home/mca/230968014_p.jaswanthkumar/week4/cats_and_dogs_filtered/validation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abb80d-316d-437a-8a99-59d57ed85510",
   "metadata": {},
   "source": [
    "# A) Implement the following CNN architectures: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "44b7b944-1ab9-444f-b853-c1f9caf6cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (128, 128)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e70b859a-83fc-4492-ab7c-0f0c1207d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c1fb115b-e1d9-482f-bb0d-301823c9b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = train_data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "efbecfc8-5d14-4859-aef6-9924f0b8576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lenet5(input_shape=(128,128,1), num_classes=2):\n",
    "    model1 = models.Sequential([\n",
    "        layers.Conv2D(6, (5,5), activation='tanh', input_shape=input_shape),\n",
    "        layers.AveragePooling2D(),\n",
    "        layers.Conv2D(16, (5,5), activation='tanh'),\n",
    "        layers.AveragePooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='tanh'),\n",
    "        layers.Dense(84, activation='tanh'),\n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6ebe5731-d585-417a-87ad-0a285fba7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alexnet(input_shape=(128,128,1), num_classes=2):\n",
    "    model2 = models.Sequential([\n",
    "        layers.Conv2D(96, (11,11), strides=4, padding='same', activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1678501e-8510-40df-ae10-ee1453a6bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs=5, lr=0.001):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    history = model.fit(train_data, validation_data=val_data, epochs=epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe2e1a-88cc-42c3-a0bd-d64af476383c",
   "metadata": {},
   "source": [
    "# B) Train, test, and compare the performance of these models on the following datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5ab0bbe0-4090-4d37-b357-8ecb57eeab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model1: LeNet-5 ...\n",
      "Epoch 1/15\n",
      "63/63 [==============================] - 3s 29ms/step - loss: 0.7176 - accuracy: 0.5120 - val_loss: 0.6868 - val_accuracy: 0.5490\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.6795 - accuracy: 0.5670 - val_loss: 0.6956 - val_accuracy: 0.5270\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6397 - accuracy: 0.6305 - val_loss: 0.7103 - val_accuracy: 0.5500\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.5983 - accuracy: 0.6740 - val_loss: 0.7276 - val_accuracy: 0.5420\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.5194 - accuracy: 0.7515 - val_loss: 0.7677 - val_accuracy: 0.5470\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.4679 - accuracy: 0.7765 - val_loss: 0.8596 - val_accuracy: 0.5400\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.3915 - accuracy: 0.8195 - val_loss: 0.9553 - val_accuracy: 0.5630\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.2973 - accuracy: 0.8820 - val_loss: 0.9594 - val_accuracy: 0.5660\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.2305 - accuracy: 0.9145 - val_loss: 1.2898 - val_accuracy: 0.5450\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.1478 - accuracy: 0.9495 - val_loss: 1.4840 - val_accuracy: 0.5470\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0949 - accuracy: 0.9710 - val_loss: 1.6088 - val_accuracy: 0.5580\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0600 - accuracy: 0.9850 - val_loss: 1.8633 - val_accuracy: 0.5410\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0296 - accuracy: 0.9960 - val_loss: 2.0762 - val_accuracy: 0.5480\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.0129 - accuracy: 0.9995 - val_loss: 2.2978 - val_accuracy: 0.5550\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0073 - accuracy: 0.9995 - val_loss: 2.4072 - val_accuracy: 0.5480\n",
      "LeNet-5 Final Validation Accuracy: 54.80%\n"
     ]
    }
   ],
   "source": [
    "model1 = build_lenet5(num_classes=num_classes)\n",
    "print(\"Training model1: LeNet-5 ...\")\n",
    "history1 = train_model(model1, train_data, val_data, epochs=15)\n",
    "val_loss1, val_acc1 = model1.evaluate(val_data, verbose=0)\n",
    "print(f\"LeNet-5 Final Validation Accuracy: {val_acc1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "03360632-b0b2-4a88-9dc3-af2f9fd249ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model2: AlexNet ...\n",
      "Epoch 1/15\n",
      "63/63 [==============================] - 3s 29ms/step - loss: 0.6943 - accuracy: 0.4880 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.6925 - accuracy: 0.5150 - val_loss: 0.6929 - val_accuracy: 0.5020\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.6832 - accuracy: 0.5565 - val_loss: 0.7753 - val_accuracy: 0.5210\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.6990 - accuracy: 0.5105 - val_loss: 0.6926 - val_accuracy: 0.5010\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.6926 - accuracy: 0.5290 - val_loss: 0.6818 - val_accuracy: 0.5620\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.6632 - accuracy: 0.6155 - val_loss: 0.6381 - val_accuracy: 0.6420\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.6217 - accuracy: 0.6700 - val_loss: 0.6318 - val_accuracy: 0.6260\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.5708 - accuracy: 0.7155 - val_loss: 0.6342 - val_accuracy: 0.6500\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.5087 - accuracy: 0.7560 - val_loss: 0.6093 - val_accuracy: 0.6910\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.4348 - accuracy: 0.8040 - val_loss: 0.5622 - val_accuracy: 0.7140\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.3681 - accuracy: 0.8340 - val_loss: 0.5996 - val_accuracy: 0.7290\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.3263 - accuracy: 0.8635 - val_loss: 0.7083 - val_accuracy: 0.6730\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.2381 - accuracy: 0.9025 - val_loss: 0.9187 - val_accuracy: 0.6790\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.1846 - accuracy: 0.9270 - val_loss: 1.0304 - val_accuracy: 0.6880\n",
      "AlexNet Final Validation Accuracy: 68.80%\n"
     ]
    }
   ],
   "source": [
    "model2 = build_alexnet(num_classes=num_classes)\n",
    "print(\"Training model2: AlexNet ...\")\n",
    "history2 = train_model(model2, train_data, val_data, epochs=15, lr=0.0001)\n",
    "val_loss2, val_acc2 = model2.evaluate(val_data, verbose=0)\n",
    "print(f\"AlexNet Final Validation Accuracy: {val_acc2*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490906ff-e7e2-4904-b054-be39ee507428",
   "metadata": {},
   "source": [
    "# Q2. Transfer Learning and Model Performance Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "85372005-c002-430b-8585-be41c19d9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_model(base_model, num_classes=2):\n",
    "    base_model.trainable = False  # freeze feature extractor\n",
    "    inputs = layers.Input(shape=(128,128,3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d667bdce-1a33-4b32-941c-f152623e1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_models = {\n",
    "    \"VGG16\": tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False, input_shape=(128,128,3)),\n",
    "    \"InceptionV3\": tf.keras.applications.InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(128,128,3)),\n",
    "    \"ResNet50\": tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_shape=(128,128,3)),\n",
    "    \"EfficientNetB0\": tf.keras.applications.EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(128,128,3)),\n",
    "    \"MobileNetV2\": tf.keras.applications.MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(128,128,3)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dc6791a3-fa65-412b-b056-46412b88c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ccb17ecf-fcca-4f0d-a760-5901d548157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transfer Model: VGG16 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 4s 50ms/step - loss: 0.6696 - accuracy: 0.6345 - val_loss: 0.6331 - val_accuracy: 0.7400\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.6029 - accuracy: 0.7705 - val_loss: 0.5750 - val_accuracy: 0.7820\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 3s 48ms/step - loss: 0.5538 - accuracy: 0.7870 - val_loss: 0.5331 - val_accuracy: 0.7950\n",
      "VGG16 Final Validation Accuracy: 79.50%\n",
      "\n",
      "Training Transfer Model: InceptionV3 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 5s 39ms/step - loss: 0.4124 - accuracy: 0.8345 - val_loss: 0.2584 - val_accuracy: 0.9080\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 2s 30ms/step - loss: 0.1863 - accuracy: 0.9245 - val_loss: 0.2359 - val_accuracy: 0.9060\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.1511 - accuracy: 0.9415 - val_loss: 0.2167 - val_accuracy: 0.9100\n",
      "InceptionV3 Final Validation Accuracy: 91.00%\n",
      "\n",
      "Training Transfer Model: ResNet50 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 4s 45ms/step - loss: 0.6973 - accuracy: 0.5045 - val_loss: 0.6916 - val_accuracy: 0.5400\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 2s 38ms/step - loss: 0.6897 - accuracy: 0.5620 - val_loss: 0.6895 - val_accuracy: 0.5440\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.6878 - accuracy: 0.5615 - val_loss: 0.6875 - val_accuracy: 0.5810\n",
      "ResNet50 Final Validation Accuracy: 58.10%\n",
      "\n",
      "Training Transfer Model: EfficientNetB0 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 5s 38ms/step - loss: 0.6964 - accuracy: 0.4850 - val_loss: 0.6944 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.6942 - accuracy: 0.5055 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.6937 - accuracy: 0.5150 - val_loss: 0.6958 - val_accuracy: 0.5000\n",
      "EfficientNetB0 Final Validation Accuracy: 50.00%\n",
      "\n",
      "Training Transfer Model: MobileNetV2 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 3s 35ms/step - loss: 0.4072 - accuracy: 0.8425 - val_loss: 0.1970 - val_accuracy: 0.9450\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.1527 - accuracy: 0.9545 - val_loss: 0.1305 - val_accuracy: 0.9590\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.1116 - accuracy: 0.9650 - val_loss: 0.1107 - val_accuracy: 0.9630\n",
      "MobileNetV2 Final Validation Accuracy: 96.30%\n"
     ]
    }
   ],
   "source": [
    "for name, base in transfer_models.items():\n",
    "    print(f\"\\nTraining Transfer Model: {name} ...\")\n",
    "    model = build_transfer_model(base, num_classes=num_classes)\n",
    "    history = train_model(model, train_data, val_data, epochs=3, lr=0.0001)\n",
    "    val_loss, val_acc = model.evaluate(val_data, verbose=0)\n",
    "    results[name] = val_acc * 100\n",
    "    print(f\"{name} Final Validation Accuracy: {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "afad935d-8d88-43b8-9ec3-6b73a9de8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Validation Accuracy ===\n",
      "LeNet-5: 54.80%\n",
      "AlexNet: 68.80%\n",
      "VGG16: 79.50%\n",
      "InceptionV3: 91.00%\n",
      "ResNet50: 58.10%\n",
      "EfficientNetB0: 50.00%\n",
      "MobileNetV2: 96.30%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Validation Accuracy ===\")\n",
    "print(f\"LeNet-5: {val_acc1*100:.2f}%\")\n",
    "print(f\"AlexNet: {val_acc2*100:.2f}%\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed97d5-9527-4282-afc8-d3cc698b0122",
   "metadata": {},
   "source": [
    "# Face Mask Detection Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27406f-ffe9-4163-a1af-0781f9fa1e4a",
   "metadata": {},
   "source": [
    "# Q1. Implementation and Comparison of CNN Architectures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b8a47939-ca53-4bb0-aa84-67519221e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c297dc70-1dfc-4113-aeb7-ca44f196d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/mca/230968014_p.jaswanthkumar/week4/Face_Mask/Face Mask Dataset/Train\"\n",
    "val_dir   = \"/home/mca/230968014_p.jaswanthkumar/week4/Face_Mask/Face Mask Dataset/Validation\"\n",
    "test_dir  = \"/home/mca/230968014_p.jaswanthkumar/week4/Face_Mask/Face Mask Dataset/Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90ba1c-20bd-4570-9b0c-6ea393c31668",
   "metadata": {},
   "source": [
    "# A) Implement the following CNN architectures: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac6965c1-5408-4961-aa39-0e3ce37fbfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size   = (128, 128)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ccc6d78d-c177-48d4-a250-14e7dc220f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2195fcb1-8899-4ba7-a073-3f0322b2e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 992 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir, target_size=img_size, batch_size=batch_size,\n",
    "    class_mode=\"categorical\", color_mode=\"grayscale\"\n",
    ")\n",
    "val_data = datagen.flow_from_directory(\n",
    "    val_dir, target_size=img_size, batch_size=batch_size,\n",
    "    class_mode=\"categorical\", color_mode=\"grayscale\"\n",
    ")\n",
    "test_data = datagen.flow_from_directory(\n",
    "    test_dir, target_size=img_size, batch_size=batch_size,\n",
    "    class_mode=\"categorical\", color_mode=\"grayscale\", shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4e9fc155-5fa0-449d-95f0-a1d4c8f4e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'WithMask': 0, 'WithoutMask': 1}\n"
     ]
    }
   ],
   "source": [
    "num_classes = train_data.num_classes\n",
    "print(\"Classes:\", train_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "60a2d95a-f8d1-4716-8a66-2bf6ad845467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lenet5(input_shape=(128,128,1), num_classes=2):\n",
    "    model1 = models.Sequential([\n",
    "        layers.Conv2D(6, (5,5), activation='tanh', input_shape=input_shape),\n",
    "        layers.AveragePooling2D(),\n",
    "        layers.Conv2D(16, (5,5), activation='tanh'),\n",
    "        layers.AveragePooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='tanh'),\n",
    "        layers.Dense(84, activation='tanh'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1540d7b-e4bf-456f-b615-1fc1f9fbc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alexnet(input_shape=(128,128,1), num_classes=2):\n",
    "    model2 = models.Sequential([\n",
    "        layers.Conv2D(96, (11,11), strides=4, padding='same', activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "03af7025-9b5d-429f-b2aa-426d640afb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs=5, lr=0.001):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    history = model.fit(train_data, validation_data=val_data, epochs=epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf606c-2f31-4b28-a506-25d97ef33feb",
   "metadata": {},
   "source": [
    "# B) Train, test, and compare the performance of these models on the following datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "35753ddb-4609-430e-904c-5b4fe93b275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model1: LeNet-5 ...\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 8s 23ms/step - loss: 0.2501 - accuracy: 0.8955 - val_loss: 0.1218 - val_accuracy: 0.9525\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.0854 - accuracy: 0.9680 - val_loss: 0.0566 - val_accuracy: 0.9812\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0880 - accuracy: 0.9679 - val_loss: 0.0614 - val_accuracy: 0.9812\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0607 - accuracy: 0.9781 - val_loss: 0.0339 - val_accuracy: 0.9937\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0442 - accuracy: 0.9838 - val_loss: 0.0682 - val_accuracy: 0.9737\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.0438 - val_accuracy: 0.9825\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0376 - accuracy: 0.9855 - val_loss: 0.0464 - val_accuracy: 0.9837\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0350 - accuracy: 0.9869 - val_loss: 0.0417 - val_accuracy: 0.9875\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0291 - accuracy: 0.9891 - val_loss: 0.0426 - val_accuracy: 0.9875\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0224 - accuracy: 0.9912 - val_loss: 0.0899 - val_accuracy: 0.9712\n",
      "LeNet-5 Final Validation Accuracy: 97.12%\n"
     ]
    }
   ],
   "source": [
    "model1 = build_lenet5(num_classes=num_classes)\n",
    "print(\"Training model1: LeNet-5 ...\")\n",
    "history1 = train_model(model1, train_data, val_data, epochs=10)\n",
    "val_loss1, val_acc1 = model1.evaluate(val_data, verbose=0)\n",
    "print(f\"LeNet-5 Final Validation Accuracy: {val_acc1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b947f65e-3f20-43dc-ae46-d230d07667f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model2: AlexNet ...\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 9s 23ms/step - loss: 0.2238 - accuracy: 0.9081 - val_loss: 0.1022 - val_accuracy: 0.9700\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0642 - accuracy: 0.9768 - val_loss: 0.0326 - val_accuracy: 0.9925\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0398 - accuracy: 0.9862 - val_loss: 0.0233 - val_accuracy: 0.9950\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0279 - accuracy: 0.9897 - val_loss: 0.0316 - val_accuracy: 0.9900\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0271 - accuracy: 0.9893 - val_loss: 0.0231 - val_accuracy: 0.9925\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0235 - accuracy: 0.9916 - val_loss: 0.0282 - val_accuracy: 0.9937\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.0261 - val_accuracy: 0.9925\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0196 - accuracy: 0.9927 - val_loss: 0.0236 - val_accuracy: 0.9937\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.1811 - val_accuracy: 0.9300\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.0170 - val_accuracy: 0.9950\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0141 - accuracy: 0.9948 - val_loss: 0.0344 - val_accuracy: 0.9937\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.0178 - val_accuracy: 0.9937\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.0404 - val_accuracy: 0.9925\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0174 - accuracy: 0.9935 - val_loss: 0.0243 - val_accuracy: 0.9937\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0084 - accuracy: 0.9968 - val_loss: 0.0382 - val_accuracy: 0.9962\n",
      "AlexNet Final Validation Accuracy: 99.62%\n"
     ]
    }
   ],
   "source": [
    "model2 = build_alexnet(num_classes=num_classes)\n",
    "print(\"Training model2: AlexNet ...\")\n",
    "history2 = train_model(model2, train_data, val_data, epochs=15, lr=0.0001)\n",
    "val_loss2, val_acc2 = model2.evaluate(val_data, verbose=0)\n",
    "print(f\"AlexNet Final Validation Accuracy: {val_acc2*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6db59-4257-473b-b57a-2683ae29beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
